{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark serverless on Colab",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yytvDEDKWXrn",
        "colab_type": "text"
      },
      "source": [
        "# Spark serverless on Colaboratory\n",
        "\n",
        "Powered by Open Data Studio [spark-serverless](https://github.com/open-datastudio/spark-serverless) project, it installs Apache Spark 3.0.0 in the serverless configuration.\n",
        "\n",
        "## How it works\n",
        "In Spark serverless configuration, the Spark driver runs in Colab, but executors are running on the cloud. So you can scale your job without managing the cluster.\n",
        "\n",
        "## Cloud platform\n",
        "Currently, the [spark-serverless](https://github.com/open-datastudio/spark-serverless) project runs executors on [staroid](https://staroid.com), the cloud platform for open-source projects. So you'll need a staroid account to try it.\n",
        "\n",
        "Contribution for other cloud platform support is welcome!\n",
        "\n",
        "## Get supported\n",
        "\n",
        "Thereâ€™s no such thing as a stupid question!\n",
        "Don't hesitate to create an [issue](https://github.com/open-datastudio/spark-serverless/issues) or join our [Slack channel](https://join.slack.com/t/opendatastudio/shared_invite/zt-fy2dsmb7-E9_UrBAh4UA47lzN5sUHUA).\n",
        "\n",
        "\n",
        "## Get involved, get funded\n",
        "\n",
        "[spark-serverless](https://github.com/open-datastudio/spark-serverless) is an MIT licensed **open-source** project, as a part of the [Open data studio](https://open-datastudio.io) project. Bugfixes, optimizations, new features, ... please feel free to create [pull requests](https://github.com/open-datastudio/spark-serverless/pulls).\n",
        "\n",
        "\n",
        "If you're listed in Contributors page of the Github repository of the project, you'll get funded by [StarRank](https://staroid.com/site/starrank) when user run spark-serverless on staroid.\n",
        "\n",
        "\n",
        "## FAQ\n",
        "\n",
        "Q: Can I install spark-serverless in my laptop or in other environment?\n",
        "\n",
        "A: Yes, It works everywhere. The same installation/configuration code in this notebook supposed to work in almost everywhere.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDt1thcgaeWK",
        "colab_type": "text"
      },
      "source": [
        "# Install\n",
        "\n",
        "## Setup Kubernetes cluster (staroid)\n",
        "\n",
        "You need to do this only once, unless you remove the cluster.\n",
        "\n",
        "  - sign in [staroid](https://staroid.com)\n",
        "  - Kubernetes -> New Kubernetes cluster\n",
        "\n",
        "## Configure spark serverless\n",
        "\n",
        "Set following environment variables\n",
        "\n",
        "  - `STAROID_ACCESS_TOKEN`\n",
        "  - `STAROID_ACCOUNT`\n",
        "  - `STAROID_CLUSTER`\n",
        "  - `SPARK_INSTANCE_NAME`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt5YWoVteDhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# get a staroid api access token from https://staroid.com/settings/accesstokens\n",
        "os.environ[\"STAROID_ACCESS_TOKEN\"] = \"<YOUR_ACCESS_TOKEN>\"\n",
        "os.environ[\"STAROID_ACCOUNT\"] = \"GITHUB/<your github account>\"\n",
        "os.environ[\"STAROID_CLUSTER\"] = \"<cluster name, created from 'Setup Kubernetes cluster' section>\"\n",
        "os.environ[\"SPARK_INSTANCE_NAME\"] = \"spark1\" # instance name of spark serverless to create. You can change as you want. [a-z0-9-]\n",
        "\n",
        "# Configure image -------------------------------------------------------\n",
        "# Followings are configured for colab. You don't have to change anything here.\n",
        "# If you'd like to use it in other environment, you can change followings accordingly.\n",
        "#\n",
        "# Spark driver and executor should use the same python version\n",
        "# 'opendatastudio/spark-py' image includes multiple pythons inside.\n",
        "# /home/spark/.pyenv/versions/3.6.9/bin/python3\n",
        "# /home/spark/.pyenv/versions/3.7.7/bin/python3\n",
        "# /home/spark/.pyenv/versions/3.8.1/bin/python3\n",
        "# set PYSPARK_PYTHON environment variable based on the python version of the driver.\n",
        "# In case of colab, we use python 3.6.\n",
        "os.environ[\"SPARK_IMAGE\"] = \"opendatastudio/spark-py:v3.0.0-staroid\"\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"/home/spark/.pyenv/versions/3.6.9/bin/python3\"\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/bin/python\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0JsZrEKXJdY",
        "colab_type": "text"
      },
      "source": [
        "## Download binaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwVhUNO5WVvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar -xf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "# staroid cli\n",
        "!curl -o starctl -s -L https://github.com/staroids/starctl/releases/download/v0.0.1/starctl-linux-amd64 && chmod +x starctl\n",
        "# kubectl cli\n",
        "!curl -o kubectl -s -L https://storage.googleapis.com/kubernetes-release/release/v1.16.3/bin/linux/amd64/kubectl && chmod +x kubectl\n",
        "# clone spark-serverless\n",
        "!git clone https://github.com/open-datastudio/spark-serverless\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\"\n",
        "os.environ[\"STARCTL\"] = \"/content/starctl\"\n",
        "os.environ[\"KUBECTL\"] = \"/content/kubectl\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-SS9ZR-Xvy6",
        "colab_type": "text"
      },
      "source": [
        "# Start spark-serverless\n",
        "\n",
        "Starts a Kubernetes namespace and secure tunnel server on the cloud. It may take few seconds to a minute.\n",
        "\n",
        "After successful start, secure tunnel is created and Kubernetes API server endpoint is available on http://localhost:8001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thf4Fpc9at5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start spark serverless\n",
        "!spark-serverless/bin/ske_start.sh -o \"$STAROID_ACCOUNT\" -c \"$STAROID_CLUSTER\" -a \"$SPARK_INSTANCE_NAME\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkmJ_iM-YIZY",
        "colab_type": "text"
      },
      "source": [
        "## Create spark session\n",
        "\n",
        "Creating spark session will create initial number of executor configured"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32SizB6r16Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate spark-defaults.conf\n",
        "!spark-serverless/bin/ske_spark_conf.sh -o \"$STAROID_ACCOUNT\" -c \"$STAROID_CLUSTER\" -a \"$SPARK_INSTANCE_NAME\" > $SPARK_HOME/conf/spark-defaults.conf\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# This will creates spark executors on Kubernetes clusters.\n",
        "# Initial run may take few minutes, in case of cluster provision new nodes for executors.\n",
        "#\n",
        "# spark.kubernetes.executor.label.pod.staroid.com/instance-type\n",
        "#\n",
        "#    Available values\n",
        "#    - 'standard-2' (2CPU, 8GB mem)\n",
        "#    - 'standard-4' (4CPU, 16GB mem)\n",
        "#    - 'standard-8' (8CPU, 32GB mem)\n",
        "#\n",
        "#    Update 'spark.executor.cores' and 'spark.executor.memory' accordingly\n",
        "#\n",
        "# spark.kubernetes.executor.label.pod.staroid.com/spot\n",
        "#\n",
        "#    'true' to locate executors on Spot instance.\n",
        "#\n",
        "spark = SparkSession.builder \\\n",
        "  .config(\"spark.executor.cores\", \"4\") \\\n",
        "  .config(\"spark.executor.memory\", \"16g\") \\\n",
        "  .config(\"spark.kubernetes.executor.label.pod.staroid.com/instance-type\", \"standard-4\") \\\n",
        "  .config(\"spark.kubernetes.executor.label.pod.staroid.com/spot\", \"true\") \\\n",
        "  .config(\"spark.dynamicAllocation.initialExecutors\", \"1\") \\\n",
        "  .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
        "  .config(\"spark.dynamicAllocation.maxExecutors\", \"10\") \\\n",
        "  .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7yxa6TJYLlx",
        "colab_type": "text"
      },
      "source": [
        "# Use spark!\n",
        "\n",
        "Initial run may take 1-5 minutes as cluster provision new nodes for spark executors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceAdjlp8EFlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl0FLizpapaQ",
        "colab_type": "text"
      },
      "source": [
        "## Open Spark UI\n",
        "\n",
        "Run following cell to get Spark UI address."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8haVIPyuEwx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "STAROID_NAMESPACE=`/content/kubectl --server localhost:8001 get pods 2>&1 | sed 's/.*system[:][^:]*[:]\\([^:]*\\).*/\\1/g'`\n",
        "STAROID_SERVICE_DOMAIN=`/content/kubectl --server localhost:8001 -n $STAROID_NAMESPACE get configmap staroid-envs -o yaml | grep STAROID_SERVICE_DOMAIN | sed 's/[^:]*..\\(.*\\)/\\1/g'`\n",
        "INSTANCE_NAME=${INSTANCE_NAME:-spark-serverless}\n",
        "\n",
        "echo \"https://p4040-spark-ui-$INSTANCE_NAME--$STAROID_SERVICE_DOMAIN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KooZVV6-k3ki",
        "colab_type": "text"
      },
      "source": [
        "# Shutdown & Clean up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKYmFYQwbC0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stop spark instance on the cloud. It does not completely destroy. So it keeps Kubernetes resources (such as ConfigMap, Secret, PersistentVolumeClaim).\n",
        "!spark-serverless/bin/ske_stop.sh -o \"$STAROID_ACCOUNT\" -c \"$STAROID_CLUSTER\" -a \"$SPARK_INSTANCE_NAME\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1QOy0DZlU12",
        "colab_type": "text"
      },
      "source": [
        "## Important\n",
        "\n",
        "Visit your Kubernetes cluster page in [staroid](https://staroid.com) and make sure the spark instance is stopped or terminated to prevent unintended usage charges.\n",
        "\n",
        "You can always stop/terminate instance from the staroid management console, if `ske_stop.sh` command above fails.\n",
        "\n"
      ]
    }
  ]
}
